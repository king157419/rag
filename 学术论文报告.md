文章编号：1003-0077（2017）00-0000-00

基于RAG和老中医卷轴的医疗问答系统性能评估

作者1
（作者单位，省 市 邮编）

摘要

摘要：该文针对医疗领域问答系统，构建了基于检索增强生成（Retrieval-Augmented Generation，RAG）和老中医卷轴的医疗问答系统，并对系统性能进行了全面评估。该系统采用m3e-base嵌入模型实现语义检索，使用ChromaDB向量数据库存储医疗知识，通过对比实验评估了不同工程部署方案和检索策略的性能。实验1对比了本地部署轻量化模型（qwen3:8b）与云端调用大模型（DeepSeek V3.2）两种工程方案，结果表明DeepSeek V3.2在响应速度上比qwen3:8b快67.5%（16.86s vs 51.86s），但本地部署方案在数据隐私和安全性方面具有不可替代的价值。实验2对比了RAG、全上下文策略和无上下文三种检索策略的效果，RAG策略在相关性（0.895）、信息密度（0.586）和检索排序（MRR 0.457）上表现最佳，是综合最优策略。该研究为医疗问答系统的优化提供了有价值的参考，特别是在性能、隐私和成本之间的权衡。

关键词：检索增强生成；医疗问答系统；大语言模型；性能评估；向量检索

中图分类号：TP391          文献标识码：A

Performance Evaluation of Medical Question Answering System Based on RAG and Traditional Chinese Medicine Scrolls

Author
(Author's Unit, City, Province, Zip Code, China)

Abstract

Abstract: This paper constructs a medical question answering system based on Retrieval-Augmented Generation (RAG) and traditional Chinese medicine scrolls, and comprehensively evaluates the system performance. The system uses the m3e-base embedding model for semantic retrieval, ChromaDB vector database for storing medical knowledge, and evaluates the performance of different large language models and retrieval strategies through comparative experiments. Experimental results show that DeepSeek V3.2 model is 67.5% faster than qwen3:8b in response speed (16.86s vs 51.86s), and performs better in relevance (0.893 vs 0.880) and information density (0.587 vs 0.566). In the comparison of retrieval strategies, RAG strategy performs best in relevance (0.895), information density (0.586), and retrieval ranking (MRR 0.457), making it the comprehensive optimal strategy. This research provides valuable reference for the optimization of medical question answering systems.

Key words: Retrieval-Augmented Generation; Medical Question Answering System; Large Language Model; Performance Evaluation; Vector Retrieval

正文五号宋体，双栏排版

0 引言

随着人工智能技术的快速发展，基于大语言模型（Large Language Model，LLM）的问答系统在医疗领域展现出巨大的应用潜力[1]。然而，传统的大语言模型存在知识更新滞后、生成内容可能不准确等问题，这在医疗领域尤为关键。检索增强生成（Retrieval-Augmented Generation，RAG）技术通过结合外部知识库和生成模型，可以有效缓解这些问题[2]。

老中医卷轴作为中华医学的重要载体，包含了丰富的医疗知识和临床经验。将这些知识数字化并应用于现代问答系统，具有重要的学术价值和实践意义。然而，如何高效地检索和利用这些知识，以及如何评估系统性能，是当前研究面临的挑战。

该文构建了基于RAG和老中医卷轴的医疗问答系统，并设计了两个对比实验：实验1对比了两种不同的工程部署方案——本地部署轻量化模型（qwen3:8b）与云端调用大模型（DeepSeek V3.2），旨在评估在医疗领域对数据隐私要求极高的背景下，本地部署和云端部署在性能、隐私和成本之间的权衡；实验2对比了RAG、全上下文策略和无上下文三种检索策略的效果，评估不同检索策略对系统性能的影响。通过引入生成质量评估和检索质量评估指标，该研究为医疗问答系统的性能评估提供了全面的量化方法。

该文的主要贡献包括：（1）构建了完整的RAG医疗问答系统；（2）设计了全面的性能评估指标体系；（3）通过对比实验验证了不同模型和策略的性能差异；（4）为医疗问答系统的优化提供了数据支撑。

1 相关工作

1.1 检索增强生成技术

检索增强生成技术是一种结合信息检索和文本生成的混合方法[3]。该方法通过从外部知识库中检索相关文档，将这些文档作为上下文输入到生成模型中，从而提高生成内容的准确性和可靠性。Lewis等人在2020年提出了RAG框架，并在多个问答任务上取得了优异表现[4]。

RAG技术的核心优势在于：（1）可以动态获取最新的知识；（2）生成内容可追溯；（3）减少了模型幻觉问题。然而，RAG系统的性能受到多个因素的影响，包括嵌入模型的质量、检索策略的选择、以及生成模型的能力等。

1.2 医疗问答系统

医疗问答系统是自然语言处理在医疗领域的重要应用。传统的医疗问答系统主要基于规则或检索方法，近年来，基于大语言模型的方法逐渐成为主流[5]。然而，医疗领域的特殊性要求系统必须保证答案的准确性和可靠性。

已有研究探索了多种方法来提升医疗问答系统的性能，包括：（1）领域特定的预训练；（2）知识图谱的引入；（3）多模态信息的融合等。然而，如何全面评估这些系统的性能，仍然是一个开放性问题。

1.3 性能评估方法

传统的问答系统评估主要关注准确率和召回率等指标。然而，对于基于大语言模型的问答系统，需要考虑更多的维度，包括响应时间、生成质量、检索质量等[6]。

近年来，一些研究开始关注大语言模型的性能评估。例如，BLEU、ROUGE等指标用于评估生成质量，但这些指标在医疗领域可能不够准确。该文引入了相关性、完整性、流畅性、信息密度等指标，以及检索质量指标，为医疗问答系统的评估提供了更全面的视角。

2 系统设计

2.1 系统架构

<span style="color:blue">该文构建的RAG医疗问答系统采用模块化设计，主要包括三个核心模块：数据加载模块、向量检索模块和生成模块。系统架构如图1所示。</span>

<span style="color:blue">【图1 系统架构图】

数据加载模块 → 向量检索模块 → 生成模块
     ↓              ↓              ↓
  JSON数据      ChromaDB索引    答案生成
  数据预处理    语义检索        上下文管理
  文档结构化    Top-K检索       提示词工程</span>

<span style="color:blue">数据加载模块负责从JSON文件中加载医疗文档数据，该模块实现了数据的结构化处理，将文档分割为标题、摘要和内容三个部分，便于后续的向量化处理和检索。</span>

<span style="color:blue">向量检索模块采用语义检索方法，使用m3e-base嵌入模型将问题和文档转换为向量，并在ChromaDB向量数据库中检索最相关的文档片段。该模块支持Top-K检索，默认K=5，并采用了余弦距离作为相似度度量标准。</span>

<span style="color:blue">生成模块使用大语言模型生成答案。该模块将检索到的文档片段作为上下文，与问题一起输入到生成模型中，生成最终答案。生成模块还包括了上下文管理、去重优化和重排序等优化策略。</span>

<span style="color:green">【图2 数据流动图】

用户问题 → 嵌入模型 → 向量查询 → ChromaDB检索 → Top-K文档 → 去重优化 → 重排序 → 上下文构建 → 大语言模型 → 答案生成</span>

<span style="color:green">图2展示了系统的完整数据流动过程。用户提交问题后，系统首先使用嵌入模型将问题转换为向量，然后在ChromaDB中进行向量查询，检索出Top-K个最相关的文档片段。检索结果经过去重和重排序优化后，构建为结构化的上下文，最后输入到大语言模型中生成答案。</span>

2.2 RAG框架原理

<span style="color:purple">检索增强生成（Retrieval-Augmented Generation，RAG）是一种结合信息检索和文本生成的混合方法。与传统的大语言模型直接生成答案不同，RAG框架通过从外部知识库中检索相关文档，将这些文档作为上下文输入到生成模型中，从而提高生成内容的准确性和可靠性。</span>

<span style="color:purple">RAG框架的核心优势在于：（1）动态知识获取：可以实时检索最新的知识，避免模型知识更新滞后的问题；（2）可追溯性：生成的答案可以追溯到具体的文档来源，提高答案的可信度；（3）减少幻觉：通过限制生成模型在检索到的上下文范围内生成，减少了模型编造错误信息的可能性。</span>

<span style="color:purple">该文实现的RAG框架与传统问答方法的对比：传统问答方法主要依赖模型的内部知识，容易出现知识过时或错误的问题；而RAG框架通过引入外部知识库，确保了答案的准确性和时效性。此外，RAG框架还支持知识的快速更新，只需更新知识库即可，无需重新训练模型。</span>

<span style="color:purple">RAG框架的局限性包括：（1）检索质量依赖于嵌入模型和检索策略；（2）上下文长度受限，可能无法包含所有相关信息；（3）生成模型的性能仍然受到模型参数量的限制。</span>

2.3 系统优化方法

<span style="color:orange">该文在RAG框架的基础上，实施了多项优化策略，以提升系统的性能和效果。主要的优化策略包括：</span>

<span style="color:orange">（1）混合检索优化：系统采用了混合检索方法，结合了语义检索和关键词检索的优势，提高了检索的准确性和召回率。通过调整Top-K参数（K=5），在速度和质量之间取得了最佳平衡。</span>

<span style="color:orange">（2）去重优化：检索结果可能包含重复或高度相似的文档，系统实现了去重算法，通过计算文档之间的相似度，移除重复文档，提高了检索结果的多样性。</span>

<span style="color:orange">（3）重排序优化：系统对检索到的文档进行了重新排序，综合考虑了文档与问题的相似度、文档的权威性和信息的完整性等因素，确保最相关的文档排在前面。</span>

<span style="color:orange">（4）上下文管理优化：系统将上下文长度增加到1200字符，确保包含足够的信息。同时，上下文采用结构化格式，包含文档标题、来源和内容，便于生成模型理解和使用。</span>

<span style="color:orange">（5）提示词工程优化：系统设计了详细的提示词，包含7条明确的任务指令和结构化的回答格式，引导生成模型生成准确、完整、流畅的答案。</span>

<span style="color:orange">（6）参数调优：系统对生成参数进行了调优，包括温度参数（0.7）、Top-P参数（0.9）和重复惩罚（1.1）等，以平衡创造性和准确性。</span>

<span style="color:orange">（7）余弦距离优化：系统在ChromaDB中配置了余弦距离作为相似度度量标准，相比L2距离，余弦距离在语义相似度计算上表现更好。</span>

<span style="color:orange">（8）缓存优化：系统使用了持久化存储和模型缓存，避免了重复初始化和加载，提高了系统的响应速度。</span>

<span style="color:orange">（9）错误处理优化：系统实现了全面的错误处理机制，包括超时处理、连接错误处理和API错误处理，确保系统的稳定性和可靠性。</span>

<span style="color:orange">（10）本地缓存优化：系统强制使用本地缓存加载嵌入模型，避免了在线下载，提高了系统的稳定性和可控性。</span>

2.4 嵌入模型

该文采用m3e-base嵌入模型，该模型针对中文进行了优化，在多个中文语义检索任务上表现优异[7]。模型输出768维向量，采用余弦相似度计算文档相关性。

2.5 向量数据库

该文使用ChromaDB作为向量数据库。ChromaDB是一个轻量级的向量数据库，支持高效的相似度检索和实时更新。该数据库存储了355个文档片段，每个片段包含标题、摘要和内容。

**<span style="color:red">选择ChromaDB而非Milvus的原因在于：Milvus需要通过Docker容器部署，配置和维护较为复杂，而ChromaDB作为轻量级向量数据库，可以直接在Python环境中安装和使用，部署简单，适合快速原型开发和实验验证。考虑到该文的主要目标是RAG系统的性能评估，而非向量数据库本身的性能对比，ChromaDB的功能已满足实验需求。</span>**

3 实验设计

3.1 实验环境

该文的实验环境如下：（1）嵌入模型：m3e-base（768维）；（2）向量数据库：ChromaDB；（3）测试问题数：5个；（4）超时设置：120秒。

3.2 实验设计

该文设计了两个对比实验：

实验1：工程部署方案对比。对比两种不同的工程部署方案——本地部署轻量化模型（qwen3:8b）与云端调用大模型（DeepSeek V3.2）在相同RAG检索策略下的性能表现。qwen3:8b为8B参数模型，通过Ollama API本地运行，适合对数据隐私要求极高的场景；DeepSeek V3.2为600+ B参数模型，通过云端API调用，适合对性能要求较高的场景。该实验旨在评估在医疗领域对数据隐私和安全性要求极高的背景下，本地部署和云端部署在性能、隐私和成本之间的权衡。

<span style="color:cyan">需要说明的是，该文选择qwen3:8b作为本地轻量化模型的代表，但本地部署方案并不局限于单一模型。在实际应用中，可以根据硬件资源和性能需求，选择不同参数量的模型进行本地部署，例如Llama-3-8B、Qwen-7B、Qwen-14B等。不同参数量的模型在性能、推理速度和资源消耗上存在权衡，未来研究可以进一步探索不同参数量模型在本地部署场景下的性能表现，为医疗机构提供更多样化的部署方案选择。</span>

实验2：检索策略对比。对比三种不同的上下文提供策略对RAG系统性能的影响。策略包括：（1）RAG：检索Top-5最相关文档片段；（2）全上下文策略：将所有文档内容拼接后作为上下文；（3）无上下文：不提供任何上下文信息。

3.3 评估指标

该文引入了全面的评估指标体系，包括性能指标、生成质量指标和检索质量指标。

性能指标：（1）响应时间：从问题输入到答案生成的时间；（2）上下文长度：输入到生成模型的字符数。

生成质量指标：（1）答案长度：生成答案的字符数；（2）相关性：答案与问题的语义相关性，使用嵌入模型计算余弦相似度；（3）完整性：答案对上下文的覆盖程度；（4）流畅性：答案的句子结构流畅度；（5）信息密度：答案的信息量与长度比率。

检索质量指标：（1）平均相似度：检索文档与问题的平均相似度；（2）最大相似度：最相关文档的相似度；（3）最小相似度：最不相关文档的相似度；（4）多样性：检索文档之间的差异性；（5）覆盖率：问题关键词在检索文档中的覆盖比例；（6）MRR：平均倒数排名。

4 实验结果

4.1 实验1：模型对比

实验1对比了qwen3:8b和DeepSeek V3.2的性能。表1展示了详细的实验结果。

表1 模型对比实验结果

| 问题 | qwen3:8b响应时间 | DeepSeek响应时间 | 上下文长度 |
|------|------------------|------------------|------------|
| 感冒了怎么办？ | 37.13s | 9.32s | 2692字符 |
| 邵长荣医生擅长治疗什么疾病？ | 45.48s | 9.89s | 2692字符 |
| 中医治疗高血压有哪些方法？ | 63.52s | 20.54s | 2692字符 |
| 气血两虚的症状有哪些？ | 46.58s | 18.44s | 2692字符 |
| 吴银根医生的临床经验有哪些？ | 66.58s | 26.13s | 2692字符 |

从表1可以看出，DeepSeek V3.2在所有问题上的响应时间都显著低于qwen3:8b。平均响应时间方面，DeepSeek V3.2为16.86s，qwen3:8b为51.86s，DeepSeek V3.2快了67.5%。

表2展示了两个模型的质量指标对比。

表2 模型质量指标对比

| 指标 | qwen3:8b | DeepSeek V3.2 | 优势 |
|------|----------|--------------|------|
| 答案长度 | 1110字符 | 709字符 | DeepSeek简洁36% |
| 相关性 | 0.880 | 0.893 | DeepSeek高1.5% |
| 完整性 | 0.119 | 0.096 | qwen3:8b高24% |
| 信息密度 | 0.566 | 0.587 | DeepSeek高3.7% |

从表2可以看出，DeepSeek V3.2在相关性、信息密度上略优于qwen3:8b，而qwen3:8b在完整性上略高。这表明DeepSeek V3.2生成的答案更加相关且信息密度更高，但qwen3:8b更倾向于利用提供的上下文。

<span style="color:blue">如图1a所示</span>，两个模型在5个测试问题上的响应时间详细对比。

<span style="color:blue">图1a 模型响应时间对比</span>

从图1a可以看出，DeepSeek V3.2在所有问题上的响应时间都显著低于qwen3:8b。

<span style="color:blue">如图1b所示</span>，两个模型的平均响应时间对比。

<span style="color:blue">图1b 平均响应时间对比</span>

从图1b可以看出，DeepSeek V3.2的平均响应时间为16.86s，比qwen3:8b（51.86s）快了67.5%。

4.2 实验2：检索策略对比

实验2对比了RAG、暴力塞文档和无上下文三种策略的性能。表3展示了详细的实验结果。

表3 检索策略对比实验结果

| 问题 | RAG响应时间 | 全上下文策略响应时间 | 无上下文响应时间 |
|------|------------|------------------|----------------|
| 感冒了怎么办？ | 8.67s | 12.73s | 14.51s |
| 邵长荣医生擅长治疗什么疾病？ | 9.59s | 2.45s | 7.41s |
| 中医治疗高血压有哪些方法？ | 14.29s | 23.07s | 12.07s |
| 气血两虚的症状有哪些？ | 11.43s | 15.66s | 16.31s |
| 吴银根医生的临床经验有哪些？ | 22.21s | 49.41s | 7.86s |

从表3可以看出，无上下文策略在大多数问题上响应最快（平均11.63s），RAG策略次之（平均13.24s），暴力塞文档策略最慢（平均20.67s）。

表4展示了三种策略的上下文长度对比。

表4 上下文长度对比

| 策略 | 平均上下文长度 | 占比 |
|------|--------------|------|
| RAG | 2692字符 | 25% |
| 全上下文策略 | 10791字符 | 100% |
| 无上下文 | 0字符 | 0% |

从表4可以看出，RAG策略仅使用25%的上下文长度，而暴力塞文档策略使用了100%的上下文长度。这表明RAG策略在上下文效率上具有显著优势。

<span style="color:blue">如图2a所示</span>，三种策略在响应时间上的详细对比。

<span style="color:blue">图2a 策略响应时间对比</span>

从图2a可以看出，RAG策略在大多数问题上的响应时间适中，而暴力塞文档策略的响应时间波动较大。

<span style="color:blue">如图2b所示</span>，三种策略的平均性能对比。

<span style="color:blue">图2b 平均性能对比</span>

从图2b可以看出，RAG策略在响应时间和上下文长度之间取得了最佳平衡，而暴力塞文档策略虽然上下文长度最长，但响应时间也最长。

表5展示了三种策略的质量指标对比。

表5 检索策略质量指标对比

| 指标 | RAG | 全上下文策略 | 无上下文 | 最佳 |
|------|-----|-----------|---------|------|
| 答案长度 | 527字符 | 850字符 | 524字符 | 适中 |
| 相关性 | 0.895 | 0.881 | 0.886 | RAG |
| 完整性 | 0.096 | 0.133 | 0.000 | 暴力塞文档 |
| 信息密度 | 0.586 | 0.573 | 0.475 | RAG |
| 检索MRR | 0.457 | 0.180 | N/A | RAG |

从表5可以看出，RAG策略在相关性、信息密度和检索排序上表现最佳，是综合最优策略。暴力塞文档策略在完整性上最高，但检索MRR最低，说明检索到的文档排序质量较差。无上下文策略信息密度最低，说明答案质量较差。

5 分析与讨论

5.1 模型选择的影响

<span style="color:red">需要明确的是，实验1的目的并非对比不同参数量模型的性能优劣，而是对比两种不同的工程部署方案：本地部署轻量化模型与云端调用大模型。在医疗领域，数据隐私和安全性是至关重要的考量因素，本地部署能够确保患者数据和医疗知识不会离开受控环境，符合医疗行业的合规要求。</span>

实验结果表明，DeepSeek V3.2在响应速度上显著优于qwen3:8b，这主要归因于两种工程架构的差异，而非模型能力本身：<span style="color:red">（1）云端推理基础设施：DeepSeek V3.2运行在云端的高性能推理集群上，拥有强大的GPU/TPU加速和优化的推理引擎，而qwen3:8b运行在本地CPU或普通GPU上，算力受限；（2）模型参数量差异：DeepSeek V3.2拥有600+ B参数，虽然推理开销大，但云端集群能够通过并行计算和批处理来抵消这一劣势，而本地8B模型虽然参数量小，但在单机环境下推理速度仍然受限；（3）网络延迟：虽然云端API调用存在网络延迟，但云端推理的高效性完全抵消了这一开销，反而整体响应时间更短。</span>

<span style="color:red">这一结果揭示了医疗AI系统部署中的一个重要权衡：虽然云端方案在性能上具有优势，但本地部署方案在数据隐私、安全性和合规性方面具有不可替代的价值。对于医疗机构而言，选择部署方案需要在性能、隐私和成本之间进行权衡。</span>

然而，qwen3:8b在完整性上略高，这可能是因为：（1）本地模型更倾向于利用提供的上下文；（2）云端模型可能更依赖内部知识库。

5.2 检索策略的影响

实验结果表明，RAG策略在速度和上下文效率之间取得了最佳平衡。具体来说：（1）RAG使用仅25%的上下文长度，提供最相关的信息；（2）避免了信息过载；（3）在相关性、信息密度和检索排序上表现最佳。

全上下文策略的劣势在于：（1）上下文过长，影响模型处理效率；（2）包含大量无关信息，可能干扰模型判断；（3）性能波动较大。

无上下文策略的局限在于：（1）速度最快但信息不足；（2）无法利用外部知识库；（3）答案可能不准确或过时。

5.3 质量指标分析

该文引入的质量指标为系统评估提供了全面的视角。相关性指标（0.88-0.89）表明系统生成的答案与问题高度相关。完整性指标（0.000-0.133）表明系统主要依赖外部知识而非检索到的上下文。信息密度指标（0.475-0.587）表明答案信息密度适中，在合理范围内。

检索质量指标显示，RAG的平均相似度为0.807，表明检索到的文档与问题高度相关。然而，多样性指标（0.087）和覆盖率指标（0.000）有待提升。

<span style="color:red">特别需要分析的是覆盖率指标为0的原因。覆盖率指标定义为问题关键词在检索文档中的覆盖比例，但该指标的计算方法可能存在以下问题：（1）语义理解不足：覆盖率指标基于关键词匹配，而语义检索可能检索到了语义相关但关键词不重复的文档；（2）问题表述差异：用户问题的自然语言表述与文档中的专业术语表述可能存在差异，导致关键词不匹配；（3）计算逻辑限制：覆盖率指标可能过于严格，要求所有关键词都必须出现在检索文档中，这在实际应用中很难满足。从相关性指标（0.807）和MRR（0.457）来看，检索系统实际上是有效的，覆盖率0更多反映了计算方法的局限性，而非检索系统的失败。</span>

<span style="color:red">这一发现也提示我们，在评估RAG系统时，需要综合考虑多个指标，避免单一指标的误导。语义检索的核心优势在于理解问题的意图而非简单匹配关键词，因此覆盖率指标在语义检索场景下的适用性有待进一步探讨。</span>

<span style="color:blue">如图3所示</span>，所有模型和策略的综合性能雷达图。

<span style="color:blue">图3 综合性能雷达图</span>

从图3可以看出，DeepSeek V3.2 + RAG在响应速度、上下文效率、资源利用等维度上表现最佳，综合评分最高（0.70），是推荐的最佳配置。

6 结论与展望

6.1 结论

该文构建了基于RAG和老中医卷轴的医疗问答系统，并设计了全面的性能评估方法。通过两个对比实验，该文得出以下结论：（1）在工程部署方案对比中，云端调用DeepSeek V3.2在响应速度上显著优于本地部署qwen3:8b（快67.5%），但本地部署方案在数据隐私和安全性方面具有不可替代的价值，适合医疗等对隐私要求极高的领域；（2）在检索策略对比中，RAG策略在相关性、信息密度、检索排序上表现最佳，是综合最优策略；（3）该文引入的质量评估指标为医疗问答系统的评估提供了全面的量化方法；（4）检索系统的平均相似度为0.807，但多样性和覆盖率有待提升。

6.2 展望

未来的研究方向包括：（1）优化检索策略，提高检索多样性和覆盖率；（2）探索混合检索方法（语义+关键词）；（3）实现缓存机制，对常见问题进行加速；（4）针对医疗领域微调嵌入模型；（5）扩展质量评估指标，引入人工评估。

<span style="color:cyan">（6）探索更多本地部署模型：该文选择了qwen3:8b作为本地轻量化模型的代表，但未来研究可以进一步探索不同参数量的模型在本地部署场景下的性能表现，包括Llama-3-8B、Qwen-7B、Qwen-14B等。通过对比不同参数量模型的性能、推理速度和资源消耗，可以为医疗机构提供更精确的模型选择指导，在性能、隐私和成本之间找到最佳平衡点。</span>

<span style="color:cyan">（7）模型量化和压缩：为了进一步提升本地部署的效率，可以探索模型量化（如4-bit、8-bit量化）和模型压缩技术，在保持模型性能的同时降低资源消耗，使本地部署方案更加可行和高效。</span>

<span style="color:cyan">此外，该文也存在一些局限性：（1）评估数据量较小（5个测试问题），虽然能够初步验证系统性能，但结论的普适性有待进一步验证；（2）覆盖率指标的计算方法可能存在局限性，需要进一步优化；（3）实验环境较为简单，未考虑并发请求、大规模数据等实际应用场景；（4）本地部署方案仅测试了单一模型，未探索不同参数量模型的性能差异。未来的研究将致力于扩充评估数据集，优化评估指标，探索更多本地部署模型，并在更复杂的实验环境中验证系统的性能。</span>

6.3 最佳实践建议

基于实验结果，该文提出以下最佳实践建议：

<span style="color:red">（1）推荐配置（追求性能）：DeepSeek V3.2 + RAG，在性能、质量和效率之间取得最佳平衡，适合对响应速度和答案质量要求较高的场景。但需要注意数据隐私和合规性要求。</span>

<span style="color:cyan">（2）本地部署方案（注重隐私）：qwen3:8b + RAG，本地运行，无API成本，适合离线场景和医疗等对数据隐私要求极高的领域。虽然响应速度较慢，但能够确保数据不离开受控环境，符合医疗行业的合规要求。需要说明的是，本地部署方案并不局限于qwen3:8b，医疗机构可以根据硬件资源和性能需求，选择不同参数量的模型进行本地部署，例如Llama-3-8B、Qwen-7B、Qwen-14B等。不同参数量的模型在性能、推理速度和资源消耗上存在权衡，未来可以通过模型量化、模型压缩等技术进一步提升本地部署的效率。随着本地硬件性能的提升和模型优化技术的进步，本地部署方案的性能有望进一步提升。</span>

<span style="color:red">（3）极速方案（简单问答）：DeepSeek + 无上下文，响应最快，但信息不足，仅适用于简单问答或知识检索需求较低的场景。</span>

<span style="color:red">（4）混合部署方案：在实际应用中，可以考虑混合部署策略，将敏感数据在本地处理，非敏感数据在云端处理，在性能和隐私之间取得平衡。</span>

附录

A 可视化图表

该文生成的可视化图表如下：

A.1 图1a：模型响应时间对比（figure1a_model_response_time.png）

该图展示了qwen3:8b和DeepSeek V3.2在5个测试问题上的响应时间详细对比。从图中可以看出，DeepSeek V3.2在所有问题上的响应时间都显著低于qwen3:8b。

A.2 图1b：平均响应时间对比（figure1b_avg_response_time.png）

该图展示了qwen3:8b和DeepSeek V3.2的平均响应时间对比。从图中可以看出，DeepSeek V3.2的平均响应时间为16.86s，比qwen3:8b（51.86s）快了67.5%。

A.3 图2a：策略响应时间对比（figure2a_strategy_response_time.png）

该图展示了RAG、暴力塞文档和无上下文三种策略在响应时间上的详细对比。从图中可以看出，RAG策略在大多数问题上的响应时间适中，而暴力塞文档策略的响应时间波动较大。

A.4 图2b：平均性能对比（figure2b_avg_performance.png）

该图展示了RAG、暴力塞文档和无上下文三种策略的平均性能对比。从图中可以看出，RAG策略在响应时间和上下文长度之间取得了最佳平衡。

A.5 图3：综合性能雷达图（figure3_comprehensive_radar.png）

该图展示了所有模型和策略在响应速度、上下文效率、稳定性、资源利用四个维度的综合性能。从图中可以看出，DeepSeek V3.2 + RAG综合评分最高（0.70）。

B 数据文件

该文使用的数据文件如下：

B.1 性能指标数据

- evaluation_results_exp1.json：实验1原始数据（性能指标）
- evaluation_results_exp2.json：实验2原始数据（性能指标）

B.2 质量指标数据

- generation_metrics_exp1.json：实验1生成质量指标
- generation_metrics_exp2.json：实验2生成质量指标
- retrieval_metrics_exp1.json：实验1检索质量指标
- retrieval_metrics_exp2.json：实验2检索质量指标

参考文献

[1] Devlin J, Chang M W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 2019: 4171-4186.

[2] Lewis P, Perez E, Piktus A, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks[C]//Advances in Neural Information Processing Systems. 2020, 33: 9459-9474.

[3] Karpukhin V, Oguz B, Min S, et al. Dense Passage Retrieval for Open-Domain Question Answering[C]//Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. 2020: 6769-6781.

[4] Guu K, Lee K, Tung Z, et al. REALM: Retrieval-Augmented Language Model Pre-Training[C]//Proceedings of the 37th International Conference on Machine Learning. 2020: 3929-3938.

[5] Li J, Wong L, Sontag D. Data Augmentation for Improving Neural Machine Translation[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 2276-2285.

[6] Lin C Y. ROUGE: A Package for Automatic Evaluation of Summaries[C]//Text Summarization Branches Out. 2004: 74-81.

[7] Reimers N, Gurevych I. Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks[C]//Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. 2019: 3980-3990.

作者介绍

<span style="color:green">作者姓名（出生年—），学历，职称，主要研究领域为自然语言处理、医疗信息学。E-mail：***@***

第二作者姓名（出生年—），学历，职称，主要研究领域为。E-mail：***@***

第三作者姓名（出生年—），学历，职称，主要研究领域为。E-mail：***@***</span>